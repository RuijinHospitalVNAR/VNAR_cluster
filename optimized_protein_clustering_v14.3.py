#!/usr/bin/env python3
# -*- coding: utf-8 -*-

"""
è›‹ç™½è´¨å¤åˆç‰©èšç±»åˆ†æè„šæœ¬
åŠŸèƒ½ï¼š
1. ä» PDB/mmCIF æ–‡ä»¶ä¸­æå–æŠ—ä½“-æŠ—åŸçš„ CÎ± åæ ‡
2. è®¡ç®—æ¥è§¦å›¾ (contact map) å’Œå‡ ä½•ç‰¹å¾
3. èšç±» (æ”¯æŒ HDBSCAN, KMeans, DBSCAN)
4. å¯è§†åŒ–èšç±»ç»“æœ (æ ‡ç­¾åˆ†å¸ƒ / ç°‡å¤§å° / t-SNE / ç‰¹å¾é‡è¦æ€§)
"""

import os
import pickle
import csv
import numpy as np
import matplotlib.pyplot as plt
from pathlib import Path
from datetime import datetime
from multiprocessing import cpu_count, Pool
from sklearn.preprocessing import StandardScaler
from sklearn.manifold import TSNE
from sklearn.decomposition import PCA
from sklearn.metrics import silhouette_score
from sklearn.cluster import KMeans, DBSCAN
import logging

from Bio.PDB import PDBParser, MMCIFParser  # Biopython è§£æè›‹ç™½ç»“æ„
import hdbscan  # é«˜çº§èšç±»ç®—æ³• HDBSCAN
from scipy.cluster.hierarchy import linkage, dendrogram
from scipy.spatial.distance import pdist, squareform
from matplotlib import patches

# ------------------------
# æ—¥å¿—é…ç½®
# ------------------------
logger = logging.getLogger(__name__)
logging.basicConfig(level=logging.INFO, format="%(asctime)s [%(levelname)s] %(message)s")


class ProteinClusterAnalyzer:
    """
    è›‹ç™½è´¨èšç±»åˆ†æå™¨ç±»
    - è¾“å…¥ï¼šç»“æ„æ–‡ä»¶ç›®å½•
    - è¾“å‡ºï¼šèšç±»ç»“æœ + å¯è§†åŒ–
    """

    def __init__(self, cif_dir, antibody_chain='A', antigen_chains=['B', 'C'],
                 dist_cutoff=5.0, n_jobs=-1):
        """
        åˆå§‹åŒ–åˆ†æå™¨

        å‚æ•°ï¼š
        - cif_dir: ç»“æ„æ–‡ä»¶ç›®å½•
        - antibody_chain: æŠ—ä½“é“¾ID
        - antigen_chains: æŠ—åŸé“¾IDåˆ—è¡¨
        - dist_cutoff: æ¥è§¦åˆ¤æ–­çš„è·ç¦»é˜ˆå€¼ï¼ˆÃ…ï¼‰
        - n_jobs: å¹¶è¡Œå¤„ç†çº¿ç¨‹æ•°ï¼ˆ-1=è‡ªåŠ¨æ£€æµ‹CPUæ•°ï¼‰
        """
        self.cif_dir = Path(cif_dir)
        self.antibody_chain = antibody_chain
        self.antigen_chains = antigen_chains
        self.dist_cutoff = dist_cutoff
        self.n_jobs = n_jobs if n_jobs != -1 else cpu_count()

        # HDBSCAN å‚æ•°ï¼ˆå¯è°ƒï¼‰
        self.min_cluster_size = 10
        self.min_samples = 5
        self.cluster_selection_epsilon = 0.1

        # æ•°æ®å­˜å‚¨
        self.contact_maps = []     # æ¯ä¸ªç»“æ„çš„æ¥è§¦å›¾
        self.feature_vectors = []  # æ¯ä¸ªç»“æ„çš„ç‰¹å¾å‘é‡
        self.file_names = []       # æ–‡ä»¶å
        self.cluster_labels = None # èšç±»æ ‡ç­¾
        self.scaler = StandardScaler()  # æ ‡å‡†åŒ–å™¨
        # ç»Ÿä¸€è‰²å¡ï¼ˆNature é£æ ¼ï¼‰
        self.palette = ['#1f77b4', '#ff7f0e', '#2ca02c', '#d62728', '#9467bd', '#8c564b']

    # --------------------
    # è‡ªé€‚åº”å‚æ•°ä¼°è®¡
    # --------------------
    def _auto_hdbscan_params(self, n_samples: int, n_features: int):
        """
        åŸºäºæ ·æœ¬æ•°ä¸ç»´åº¦çš„ç®€å•å¯å‘å¼ä¼°è®¡ HDBSCAN å‚æ•°ã€‚
        - min_cluster_size: çº¦ n/20ï¼Œåœ¨ [5, 100] ä¹‹é—´æˆªæ–­
        - min_samples: çº¦ log2(n)ï¼Œä¸è¶…è¿‡ min_cluster_sizeï¼Œä¸” >=3
        """
        min_cluster_size = max(5, min(100, max(1, n_samples // 20)))
        min_samples = max(3, min(min_cluster_size, int(np.log2(max(n_samples, 2))) + 1))
        return min_cluster_size, min_samples

    def _auto_dbscan_params(self, X: np.ndarray):
        """
        é€šè¿‡å­é‡‡æ · + k-è·ç¦»ä¸­ä½æ•°ä¼°è®¡ epsï¼›k = log2(n) + 1ï¼Œmin_samples = kã€‚
        è‹¥ä¼°è®¡å¤±è´¥ï¼Œå›é€€åˆ° (eps=0.5, min_samples=5)ã€‚
        """
        try:
            n_samples = len(X)
            if n_samples < 10:
                return 0.5, 5
            k = max(3, int(np.log2(n_samples)) + 1)
            sample_size = min(1000, n_samples)
            rng = np.random.default_rng(42)
            idx = rng.choice(n_samples, size=sample_size, replace=False)
            S = X[idx]
            # è·ç¦»çŸ©é˜µ
            diffs = S[:, None, :] - S[None, :, :]
            dists = np.linalg.norm(diffs, axis=2)
            # æ’é™¤è‡ªè·ç¦»
            np.fill_diagonal(dists, np.inf)
            # ç¬¬ k é‚»è¿‘è·ç¦»çš„ä¸­ä½æ•°ä½œä¸º eps ä¼°è®¡
            kth = np.partition(dists, kth=k-1, axis=1)[:, k-1]
            eps = float(np.median(kth))
            eps = max(1e-6, eps)
            return eps, k
        except Exception as e:
            logger.warning(f"Auto DBSCAN parameter estimation failed: {e}")
            return 0.5, 5

    # ==========================================================
    # (1) æå– CÎ± åæ ‡å’Œæ®‹åŸºä¿¡æ¯
    # ==========================================================
    def extract_ca_coords_with_residues(self, structure_file, chain_ids):
        """
        æå–æŒ‡å®šé“¾çš„ CÎ± åŸå­åæ ‡å’Œæ®‹åŸºä¿¡æ¯
        è¿”å›ï¼š(åæ ‡æ•°ç»„, æ®‹åŸºä¿¡æ¯åˆ—è¡¨, é“¾é•¿åº¦å­—å…¸)
        """
        try:
            # åˆ¤æ–­æ–‡ä»¶æ ¼å¼ï¼šPDB æˆ– mmCIF
            ext = str(structure_file).split('.')[-1].lower()
            parser = PDBParser(QUIET=True) if ext == "pdb" else MMCIFParser(QUIET=True)
            structure = parser.get_structure('model', structure_file)

            ca_coords, residue_infos = [], []
            chain_info, found_chains = {}, set()

            # éå†æ‰€æœ‰é“¾
            for model in structure:
                for chain in model:
                    if chain.id in chain_ids:
                        found_chains.add(chain.id)
                        chain_coords, chain_residues = [], []
                        for res in chain:
                            if 'CA' in res:  # åªå– CÎ±
                                coord = res['CA'].get_coord()
                                resname = res.get_resname().strip()
                                resid = res.id[1]
                                chain_coords.append(coord)
                                chain_residues.append((resname, resid, chain.id))
                        if chain_coords:
                            ca_coords.extend(chain_coords)
                            residue_infos.extend(chain_residues)
                            chain_info[chain.id] = len(chain_coords)

            # æ£€æŸ¥æ˜¯å¦ç¼ºå°‘é“¾
            missing = set(chain_ids) - found_chains
            if missing:
                logger.warning(f"Chains {missing} not found in {structure_file}")

            return np.array(ca_coords), residue_infos, chain_info
        except Exception as e:
            logger.warning(f"Error processing {structure_file}: {e}")
            return np.array([]), [], {}

    # ==========================================================
    # (2) è®¡ç®—æ¥è§¦å›¾
    # ==========================================================
    def compute_contact_map_with_residues(self, coords_ab, coords_ag, residues_ab, residues_ag):
        """
        è¾“å…¥æŠ—ä½“/æŠ—åŸåæ ‡ï¼Œè®¡ç®—æ¥è§¦å›¾
        è¿”å›ï¼š(æ¥è§¦å›¾, æ¥è§¦åˆ°çš„æŠ—ä½“æ®‹åŸº, æ¥è§¦åˆ°çš„æŠ—åŸæ®‹åŸº)
        """
        if len(coords_ab) == 0 or len(coords_ag) == 0:
            return np.array([]), [], []

        # è®¡ç®—ä¸¤ä¸¤è·ç¦»
        dists = np.linalg.norm(coords_ab[:, None, :] - coords_ag[None, :, :], axis=-1)
        contact_map = (dists < self.dist_cutoff).astype(np.float32)

        rows, cols = np.where(contact_map > 0)
        contact_ab_residues = [residues_ab[i][0] for i in rows]
        contact_ag_residues = [residues_ag[i][0] for i in cols]

        return contact_map, contact_ab_residues, contact_ag_residues

    # ==========================================================
    # (3) æå–å‡ ä½•ç‰¹å¾
    # ==========================================================
    def extract_interaction_features(self, contact_map, contact_ab_residues, contact_ag_residues):
        """
        ä»æ¥è§¦å›¾æå–å‡ ä½•ç»Ÿè®¡ç‰¹å¾
        è¿”å›ï¼šå›ºå®šé•¿åº¦çš„ç‰¹å¾å‘é‡
        """
        if contact_map.size == 0:
            return np.zeros(15)

        features = [
            np.sum(contact_map), np.mean(contact_map), np.std(contact_map)  # æ€»æ•°/å‡å€¼/æ–¹å·®
        ]
        row_sums, col_sums = np.sum(contact_map, axis=1), np.sum(contact_map, axis=0)
        features.extend([
            np.mean(row_sums) if len(row_sums) else 0,
            np.std(row_sums) if len(row_sums) else 0,
            np.max(row_sums) if len(row_sums) else 0,
            np.mean(col_sums) if len(col_sums) else 0,
            np.std(col_sums) if len(col_sums) else 0,
            np.max(col_sums) if len(col_sums) else 0,
        ])

        if np.sum(contact_map) > 0:
            rows, cols = np.where(contact_map > 0)
            features.extend([
                np.std(rows) if len(rows) else 0,
                np.std(cols) if len(cols) else 0,
                len(np.unique(rows)), len(np.unique(cols))
            ])
        else:
            features.extend([0, 0, 0, 0])

        if contact_map.shape[0] > 1 and contact_map.shape[1] > 1:
            row_cont = np.sum(contact_map[:-1, :] * contact_map[1:, :])
            col_cont = np.sum(contact_map[:, :-1] * contact_map[:, 1:])
            features.extend([row_cont, col_cont])
        else:
            features.extend([0, 0])

        return np.array(features, dtype=np.float32)

    # ==========================================================
    # (4) æ–‡ä»¶å¤„ç†ä¸ç¼“å­˜
    # ==========================================================
    def process_single_file(self, structure_file):
        """
        å¤„ç†å•ä¸ªç»“æ„æ–‡ä»¶
        è¿”å›ï¼š(æ¥è§¦å›¾flatten, ç‰¹å¾å‘é‡, æ–‡ä»¶å)
        """
        try:
            ab_coords, ab_residues, _ = self.extract_ca_coords_with_residues(structure_file, [self.antibody_chain])
            ag_coords, ag_residues, _ = self.extract_ca_coords_with_residues(structure_file, self.antigen_chains)

            if len(ab_coords) == 0 or len(ag_coords) == 0:
                return None, None, None

            contact_map, ab_res, ag_res = self.compute_contact_map_with_residues(
                ab_coords, ag_coords, ab_residues, ag_residues)

            if contact_map.size == 0:
                return None, None, None

            features = self.extract_interaction_features(contact_map, ab_res, ag_res)
            return contact_map.flatten(), features, Path(structure_file).name
        except Exception as e:
            logger.error(f"Error processing {structure_file}: {e}")
            return None, None, None

    def load_and_process_data(self, cache_file="protein_data_cache.pkl"):
        """
        åŠ è½½æ•°æ®ï¼ˆæ”¯æŒç¼“å­˜ï¼‰
        - è‹¥ç¼“å­˜æ–‡ä»¶å­˜åœ¨ï¼Œç›´æ¥è¯»å–
        - å¦åˆ™é‡æ–°å¤„ç†ç»“æ„æ–‡ä»¶å¹¶å­˜å‚¨ç¼“å­˜
        """
        if Path(cache_file).exists():
            with open(cache_file, "rb") as f:
                cached = pickle.load(f)
            # å…¼å®¹ä¸¤ç§ç¼“å­˜æ ¼å¼ï¼š
            # 1) æ—§ç‰ˆ: (contact_maps, feature_vectors, file_names)
            # 2) æ–°ç‰ˆ: { 'contact_maps': ..., 'feature_vectors': ..., 'file_names': ... }
            if isinstance(cached, dict):
                self.contact_maps = cached.get("contact_maps", [])
                self.feature_vectors = cached.get("feature_vectors", [])
                self.file_names = cached.get("file_names", [])
            elif isinstance(cached, (list, tuple)) and len(cached) == 3:
                self.contact_maps, self.feature_vectors, self.file_names = cached
            else:
                logger.error(f"Unrecognized cache format in {cache_file}. Ignoring cache.")
                self.contact_maps, self.feature_vectors, self.file_names = [], [], []
                return False
            logger.info(f"Loaded cached data from {cache_file} (files: {len(self.file_names)})")
            if not self.file_names:
                logger.error("Cache is empty. Please remove cache and re-run processing.")
                return False
            return True

        # æ ¡éªŒè¾“å…¥ç›®å½•
        if not self.cif_dir.exists() or not self.cif_dir.is_dir():
            try:
                abs_path = self.cif_dir.resolve()
            except Exception:
                abs_path = str(self.cif_dir)
            logger.error(f"Structure directory not found or not a directory: {abs_path}")
            return False

        # ä»…å¤„ç†ç»“æ„æ–‡ä»¶
        structure_files = list(self.cif_dir.glob("*.cif")) + list(self.cif_dir.glob("*.pdb"))
        try:
            abs_path = self.cif_dir.resolve()
        except Exception:
            abs_path = str(self.cif_dir)
        logger.info(f"Found {len(structure_files)} structure files in {abs_path}")

        if len(structure_files) == 0:
            logger.error("No CIF or PDB files found. Please check the input directory.")
            return False

        # å¹¶è¡Œå¤„ç†ç»“æ„æ–‡ä»¶
        with Pool(processes=self.n_jobs) as pool:
            results = list(pool.imap_unordered(self.process_single_file, structure_files))

        # æ”¶é›†æœ‰æ•ˆç»“æœ
        for cmap, feat, fname in results:
            if feat is not None:
                self.contact_maps.append(cmap)
                self.feature_vectors.append(feat)
                self.file_names.append(fname)

        if len(self.file_names) == 0:
            logger.error("No valid results obtained from provided structure files.")
            return False

        with open(cache_file, "wb") as f:
            pickle.dump((self.contact_maps, self.feature_vectors, self.file_names), f)
        logger.info(f"Processed and cached {len(self.file_names)} files")
        return True

    # ==========================================================
    # (5) ç‰¹å¾é¢„å¤„ç†
    # ==========================================================
    def prepare_features(self, feature_type='combined', use_pca=True, n_components=20):
        """
        æ ‡å‡†åŒ– + å¯é€‰PCAé™ç»´
        """
        if not self.feature_vectors:
            logger.error("No engineered features available. Did data loading/processing fail?")
            return np.array([])

        X = np.array(self.feature_vectors)
        if X.size == 0:
            logger.error("Feature vector list is empty after conversion.")
            return np.array([])
        X_scaled = self.scaler.fit_transform(X)
        if use_pca and X.shape[1] > n_components:
            pca = PCA(n_components=n_components, random_state=42)
            X_scaled = pca.fit_transform(X_scaled)
        return X_scaled

    # ==========================================================
    # (6) èšç±»æ–¹æ³• (å¯åˆ‡æ¢)
    # ==========================================================
    def perform_clustering(self, X, method='hdbscan', **kwargs):
        """
        æ‰§è¡Œèšç±»
        æ”¯æŒæ–¹æ³•ï¼šhdbscan / kmeans / dbscan
        """
        n_samples = len(X)
        n_features = X.shape[1] if X.ndim == 2 else 1

        if method == 'hdbscan':
            # è‡ªé€‚åº”å‚æ•°ï¼ˆå¯è¢« kwargs è¦†ç›–ï¼‰
            auto_min_cluster_size, auto_min_samples = self._auto_hdbscan_params(n_samples, n_features)
            min_cluster_size = kwargs.get('min_cluster_size', auto_min_cluster_size)
            min_samples = kwargs.get('min_samples', auto_min_samples)
            cluster_selection_epsilon = kwargs.get('cluster_selection_epsilon', self.cluster_selection_epsilon)
            logger.info(f"HDBSCAN params -> min_cluster_size={min_cluster_size}, min_samples={min_samples}, eps={cluster_selection_epsilon}")
            clusterer = hdbscan.HDBSCAN(
                min_cluster_size=min_cluster_size,
                min_samples=min_samples,
                cluster_selection_epsilon=cluster_selection_epsilon,
                core_dist_n_jobs=self.n_jobs,
                **{k: v for k, v in kwargs.items() if k not in ['min_cluster_size','min_samples','cluster_selection_epsilon']}
            )
            self.cluster_labels = clusterer.fit_predict(X)

        elif method == 'kmeans':
            n_clusters = kwargs.get("n_clusters")
            if n_clusters is None:
                # è‡ªåŠ¨é€‰æ‹©ç°‡æ•°ï¼ˆsilhouette æœ€å¤§ï¼‰
                max_k = min(20, max(2, n_samples // 5))
                best_k, best_score = None, -1.0
                for k in range(2, max_k + 1):
                    km = KMeans(n_clusters=k, random_state=42, n_init=10)
                    labels = km.fit_predict(X)
                    # éœ€è¦è‡³å°‘ 2 ä¸ªç°‡ä¸”æ¯ç°‡è‡³å°‘æœ‰ 2 ä¸ªç‚¹
                    if len(set(labels)) > 1 and min(np.bincount(labels)) >= 2:
                        try:
                            score = silhouette_score(X, labels)
                            if score > best_score:
                                best_k, best_score = k, score
                        except Exception:
                            pass
                n_clusters = best_k if best_k is not None else 5
                logger.info(f"Auto-selected KMeans n_clusters={n_clusters} (best silhouette={best_score:.4f})")
            else:
                logger.info(f"KMeans n_clusters={n_clusters} (provided)")
            clusterer = KMeans(n_clusters=n_clusters, random_state=42, n_init=10)
            self.cluster_labels = clusterer.fit_predict(X)

        elif method == 'dbscan':
            eps = kwargs.get("eps")
            min_samples = kwargs.get("min_samples")
            if eps is None or min_samples is None:
                auto_eps, auto_min_samples = self._auto_dbscan_params(X)
                eps = auto_eps if eps is None else eps
                min_samples = auto_min_samples if min_samples is None else min_samples
                logger.info(f"Auto DBSCAN params -> eps={eps:.6f}, min_samples={min_samples}")
            else:
                logger.info(f"DBSCAN params -> eps={eps}, min_samples={min_samples} (provided)")
            clusterer = DBSCAN(eps=eps, min_samples=min_samples, n_jobs=self.n_jobs)
            self.cluster_labels = clusterer.fit_predict(X)
        else:
            raise ValueError(f"Unsupported clustering method {method}")
        return self.cluster_labels

    # ==========================================================
    # (7) èšç±»è¯„ä¼°
    # ==========================================================
    def evaluate_clustering(self, X):
        """
        ä½¿ç”¨è½®å»“ç³»æ•° (silhouette) è¯„ä¼°èšç±»è´¨é‡
        """
        if self.cluster_labels is None or len(set(self.cluster_labels)) <= 1:
            return {"silhouette": None}
        score = silhouette_score(X, self.cluster_labels)
        return {"silhouette": score}

    # ==========================================================
    # (8) ä¿å­˜ç»“æœ
    # ==========================================================
    def save_results(self, filename, X):
        results = {
            "file_names": self.file_names,
            "labels": self.cluster_labels,
            "features": X
        }
        with open(filename, "wb") as f:
            pickle.dump(results, f)
        logger.info(f"Results saved to {filename}")

        # åŒæ­¥ä¿å­˜CSVæ‘˜è¦ï¼ˆæ–‡ä»¶åä¸èšç±»æ ‡ç­¾ï¼‰
        try:
            csv_path = Path(filename).with_suffix('.csv')
            with open(csv_path, 'w', newline='') as cf:
                writer = csv.writer(cf)
                writer.writerow(["file_name", "label"])
                for fn, lb in zip(self.file_names, self.cluster_labels):
                    writer.writerow([fn, lb])
            logger.info(f"CSV summary saved to {csv_path}")
        except Exception as e:
            logger.warning(f"Failed to save CSV summary: {e}")

    # ==========================================================
    # (9) è·å–ç°‡ä»£è¡¨
    # ==========================================================
    def get_cluster_representatives(self, X):
        """
        è·å–æ¯ä¸ªç°‡çš„ä»£è¡¨æ€§ç»“æ„ï¼ˆæœ€é è¿‘ç°‡ä¸­å¿ƒçš„æ ·æœ¬ï¼‰
        """
        reps = {}
        labels = np.unique(self.cluster_labels)
        for label in labels:
            if label == -1:  # -1 ä»£è¡¨å™ªå£°
                continue
            indices = np.where(self.cluster_labels == label)[0]
            cluster_feats = X[indices]
            center = cluster_feats.mean(axis=0)
            dists = np.linalg.norm(cluster_feats - center, axis=1)
            sorted_idx = indices[np.argsort(dists)]
            reps[label] = {
                "size": len(indices),
                "files": [self.file_names[i] for i in sorted_idx[:5]],
                "distances": [dists[j] for j in np.argsort(dists)[:5]]
            }
        return reps

    # ==========================================================
    # (10) å¯è§†åŒ–
    # ==========================================================
    def visualize_results(self, X, save_path=None, show_plot=False):
        """
        ç”Ÿæˆ 4 ä¸ªå­å›¾ï¼š
        1. èšç±»æ ‡ç­¾åˆ†å¸ƒ
        2. ç°‡å¤§å°åˆ†å¸ƒ
        3. t-SNE å¯è§†åŒ–
        4. ç‰¹å¾é‡è¦æ€§
        """
        if self.cluster_labels is None:
            raise ValueError("No clustering results found.")

        try:
            plt.style.use('seaborn-v0_8')
        except:
            plt.style.use('default')

        fig, axes = plt.subplots(2, 2, figsize=(16, 12))
        fig.suptitle('Protein Complex Clustering Results', fontsize=16)

        unique_labels = np.unique(self.cluster_labels)
        # é¢œè‰²æ˜ å°„ï¼šéå™ªå£°æŒ‰è‰²å¡ï¼Œå™ªå£°(-1)ä¸ºé»‘è‰²
        non_noise_labels = [l for l in unique_labels if l != -1]
        label_to_color = {l: self.palette[i % len(self.palette)] for i, l in enumerate(non_noise_labels)}
        if -1 in unique_labels:
            label_to_color[-1] = '#000000'

        # 1. èšç±»æ ‡ç­¾åˆ†å¸ƒ
        ax1 = axes[0, 0]
        for label in unique_labels:
            color = label_to_color[label]
            mask = self.cluster_labels == label
            indices = np.where(mask)[0]
            ax1.scatter(indices, [label] * len(indices),
                        c=[color], alpha=0.7, s=30,
                        label=f'Cluster {label}' if label != -1 else 'Noise')
        ax1.set_title('Cluster Assignment')

        # 2. èšç±»å¤§å°åˆ†å¸ƒï¼ˆæ‰‹é£ç´å›¾ï¼Œæ¨ªè½´æ˜¾ç¤º 1..Kï¼‰
        ax2 = axes[0, 1]
        cluster_ids = [l for l in unique_labels if l != -1]
        cluster_sizes = [int(np.sum(self.cluster_labels == l)) for l in cluster_ids]
        if cluster_sizes:
            x_ticks = list(range(1, len(cluster_ids) + 1))
            # æ‰‹é£ç´ï¼šæ¯ä¸ªç°‡ç»˜åˆ¶ä¸€ä¸ªçŸ©å½¢æ¡ï¼Œæ¡å®½ç›¸åŒï¼Œé«˜åº¦ä¸ºæ ·æœ¬æ•°ï¼Œæ¡ä¹‹é—´ç•™è–„é—´éš”
            total_w = len(cluster_ids)
            bar_w = 0.8
            for i, (cid, sz) in enumerate(zip(cluster_ids, cluster_sizes), start=1):
                rect = patches.Rectangle((i - bar_w/2, 0), bar_w, sz,
                                         facecolor=label_to_color.get(int(cid), self.palette[int(cid) % len(self.palette)]),
                                         edgecolor='none', alpha=0.9)
                ax2.add_patch(rect)
            ax2.set_xlim(0.5, len(cluster_ids) + 0.5)
            ax2.set_ylim(0, max(cluster_sizes) * 1.05)
            ax2.set_xticks(x_ticks)
            ax2.set_xticklabels([str(i) for i in x_ticks])
            ax2.set_xlabel('Cluster ID')
            ax2.set_ylabel('Size')
            ax2.set_title('Cluster Size (Accordion)')

        # 3. t-SNE å¯è§†åŒ–
        ax3 = axes[1, 0]
        if len(X) > 1:
            tsne = TSNE(n_components=2, random_state=42, perplexity=min(30, max(5, len(X)-1)))
            X_tsne = tsne.fit_transform(X)
            for label in unique_labels:
                color = label_to_color[label]
                mask = self.cluster_labels == label
                ax3.scatter(X_tsne[mask, 0], X_tsne[mask, 1], c=[color], alpha=0.7, s=30)
            ax3.set_title('t-SNE Visualization')

        # 4. ç‰¹å¾é‡è¦æ€§ï¼ˆæ–¹å·®æ’åï¼‰
        ax4 = axes[1, 1]
        feature_matrix = np.array(self.feature_vectors)
        feature_variance = np.var(feature_matrix, axis=0)
        sorted_indices = np.argsort(feature_variance)[::-1][:15]
        # å®šä¹‰ç‰¹å¾åç§°ï¼ˆ15ä¸ªå·¥ç¨‹ç‰¹å¾ï¼‰
        default_feature_names = [
            'Total_Contacts', 'Contact_Density', 'Contact_Variability',
            'Ab_Mean_Contacts', 'Ab_Std_Contacts', 'Ab_Max_Contacts',
            'Ag_Mean_Contacts', 'Ag_Std_Contacts', 'Ag_Max_Contacts',
            'Ab_Region_Dispersion', 'Ag_Region_Dispersion',
            'Ab_Residue_Count', 'Ag_Residue_Count',
            'Ab_Continuity', 'Ag_Continuity'
        ]
        if feature_matrix.shape[1] == len(default_feature_names):
            feature_names = default_feature_names
        else:
            feature_names = [f'Feature_{i}' for i in range(feature_matrix.shape[1])]
        ax4.barh(range(len(sorted_indices)), feature_variance[sorted_indices], color="skyblue")
        ax4.set_yticks(range(len(sorted_indices)))
        ax4.set_yticklabels([feature_names[i] for i in sorted_indices])
        ax4.set_title('Top 15 Feature Importance')

        plt.tight_layout()
        if save_path:
            plt.savefig(save_path, dpi=300, bbox_inches='tight')
            logger.info(f"Visualization saved to {save_path}")
        if show_plot:
            plt.show()
        else:
            plt.close(fig)

        # å¯¼å‡ºå•å›¾ SVG ä¸æ•°æ® CSV
        if save_path:
            base_dir = Path(save_path).with_suffix("").parent
            base_name = Path(save_path).stem.replace("_clustering", "")
            # ç›®å½•
            fig_dir = base_dir / "figures"
            data_dir = base_dir / "data"
            fig_dir.mkdir(parents=True, exist_ok=True)
            data_dir.mkdir(parents=True, exist_ok=True)

            # 1) Cluster Assignment å•å›¾ + CSV
            fig1, ax = plt.subplots(figsize=(8, 6))
            for label in unique_labels:
                color = label_to_color[label]
                mask = self.cluster_labels == label
                indices = np.where(mask)[0]
                ax.scatter(indices, [label] * len(indices), c=[color], alpha=0.7, s=30)
            ax.set_title('Cluster Assignment')
            ax.set_xlabel('Sample Index')
            ax.set_ylabel('Cluster Label')
            fig1.savefig(fig_dir / f"{base_name}_cluster_assignment.svg", format='svg', bbox_inches='tight')
            plt.close(fig1)
            # CSV
            assignment_csv = data_dir / f"{base_name}_cluster_assignment.csv"
            with open(assignment_csv, 'w', newline='') as cf:
                writer = csv.writer(cf)
                writer.writerow(["index", "label"])
                for i, lb in enumerate(self.cluster_labels):
                    writer.writerow([i, int(lb)])

            # 2) Cluster Size å•å›¾ + CSV
            fig2, ax = plt.subplots(figsize=(8, 6))
            bar_colors = [label_to_color.get(int(cid), self.palette[int(cid) % len(self.palette)]) for cid in cluster_ids]
            ax.bar([str(cid) for cid in cluster_ids], cluster_sizes, color=bar_colors)
            ax.set_title('Cluster Size Distribution')
            ax.set_xlabel('Cluster ID')
            ax.set_ylabel('Size')
            fig2.savefig(fig_dir / f"{base_name}_cluster_sizes.svg", format='svg', bbox_inches='tight')
            plt.close(fig2)
            size_csv = data_dir / f"{base_name}_cluster_sizes.csv"
            with open(size_csv, 'w', newline='') as cf:
                writer = csv.writer(cf)
                writer.writerow(["cluster_id", "size"])
                for cid, sz in zip(cluster_ids, cluster_sizes):
                    writer.writerow([int(cid), int(sz)])

            # 3) t-SNE å•å›¾ + CSV
            if len(X) > 1:
                fig3, ax = plt.subplots(figsize=(8, 6))
                for label in unique_labels:
                    color = label_to_color[label]
                    mask = self.cluster_labels == label
                    ax.scatter(X_tsne[mask, 0], X_tsne[mask, 1], alpha=0.7, s=30, label=str(label), c=color)
                ax.set_title('t-SNE Visualization')
                ax.set_xlabel('t-SNE 1')
                ax.set_ylabel('t-SNE 2')
                ax.legend()
                fig3.savefig(fig_dir / f"{base_name}_tsne.svg", format='svg', bbox_inches='tight')
                plt.close(fig3)
                tsne_csv = data_dir / f"{base_name}_tsne.csv"
                with open(tsne_csv, 'w', newline='') as cf:
                    writer = csv.writer(cf)
                    writer.writerow(["index", "tsne1", "tsne2", "label"])
                    for i in range(len(X)):
                        writer.writerow([i, float(X_tsne[i,0]), float(X_tsne[i,1]), int(self.cluster_labels[i])])

            # 4) Feature Importance å•å›¾ + CSV
            fig4, ax = plt.subplots(figsize=(8, 6))
            ax.barh(range(len(sorted_indices)), feature_variance[sorted_indices], color=self.palette[0])
            ax.set_title('Top 15 Feature Importance')
            ax.set_xlabel('Variance')
            ax.set_yticks(range(len(sorted_indices)))
            ax.set_yticklabels([feature_names[i] for i in sorted_indices])
            fig4.savefig(fig_dir / f"{base_name}_feature_importance.svg", format='svg', bbox_inches='tight')
            plt.close(fig4)
            feat_csv = data_dir / f"{base_name}_feature_importance.csv"
            with open(feat_csv, 'w', newline='') as cf:
                writer = csv.writer(cf)
                writer.writerow(["feature", "variance"])
                for i in sorted_indices:
                    writer.writerow([feature_names[i], float(feature_variance[i])])

    def plot_cluster_radial_tree(self, X, save_path=None, show_plot=False):
        """
        åŸºäºèšç±»è´¨å¿ƒçš„å±‚æ¬¡èšç±»ï¼Œç»˜åˆ¶å¾„å‘æ ‘å›¾ï¼ˆå‚è€ƒå›¾ç‰‡é£æ ¼ï¼‰ã€‚
        - å¯¹æ¯ä¸ªèšç±»ï¼ˆæ’é™¤ -1ï¼‰è®¡ç®—è´¨å¿ƒ
        - å¯¹è´¨å¿ƒåšå±‚æ¬¡èšç±»ï¼ˆwardï¼‰
        - å°†æ ‘åœ¨æåæ ‡ä¸‹å¾„å‘å±•å¼€ï¼Œä¼˜åŒ–è§’åº¦åˆ†å¸ƒå’Œç°‡æ ‡ç­¾
        åŒæ—¶å¯¼å‡º SVG å’Œé“¾æ¥çŸ©é˜µ CSVã€‚
        """
        labels = np.array(self.cluster_labels)
        valid_mask = labels != -1
        if not np.any(valid_mask):
            logger.warning("No valid clusters to plot radial tree.")
            return
        
        # è®¡ç®—æ¯ä¸ªèšç±»è´¨å¿ƒ
        unique_labels = np.unique(labels[valid_mask])
        centroids = []
        cluster_sizes = []
        for lab in unique_labels:
            cluster_mask = labels == lab
            centroids.append(X[cluster_mask].mean(axis=0))
            cluster_sizes.append(np.sum(cluster_mask))
        centroids = np.vstack(centroids)

        # å±‚æ¬¡èšç±»
        Z = linkage(centroids, method='ward')
        
        # ç”Ÿæˆdendrogramè·å–å¶èŠ‚ç‚¹é¡ºåº
        fig_temp, ax_temp = plt.subplots(figsize=(1, 1))
        dendro = dendrogram(Z, orientation='right', no_labels=True, ax=ax_temp)
        plt.close(fig_temp)
        
        leaves = dendro['leaves']
        n_clusters = len(leaves)
        
        # ä¼˜åŒ–è§’åº¦åˆ†å¸ƒï¼šä»90åº¦å¼€å§‹ï¼Œé€†æ—¶é’ˆæ’åˆ—ï¼Œç¡®ä¿ç°‡é—´è§’åº¦å‡åŒ€
        start_angle = np.pi/2  # 90åº¦å¼€å§‹
        angles = np.linspace(start_angle, start_angle - 2*np.pi, n_clusters, endpoint=False)
        
        # åˆ›å»ºè§’åº¦åˆ°æ ‡ç­¾çš„æ˜ å°„
        angle_to_label = {angles[i]: unique_labels[leaves[i]] for i in range(n_clusters)}
        
        # è®¾ç½®éšæœºç§å­ç¡®ä¿èŠ‚ç‚¹åˆ†å¸ƒä¸€è‡´
        np.random.seed(42)
        
        # ç»˜åˆ¶å¾„å‘æ ‘
        fig = plt.figure(figsize=(10, 10))
        ax = fig.add_subplot(111, projection='polar')
        
        # è®¾ç½®æåæ ‡ç½‘æ ¼
        ax.set_rticks([])  # éšè—åŠå¾„åˆ»åº¦
        ax.set_thetagrids([])  # éšè—è§’åº¦åˆ»åº¦
        ax.grid(False)
        
        # ç»˜åˆ¶è¿æ¥çº¿ï¼ˆä»ä¸­å¿ƒåˆ°å¶èŠ‚ç‚¹ï¼‰
        center_radius = 0.1
        leaf_radius = 0.8
        
        for i, angle in enumerate(angles):
            # è¿æ¥çº¿
            ax.plot([0, angle], [center_radius, leaf_radius], 
                   color='#666666', linewidth=1.5, alpha=0.7)
            
            # æ ¹æ®ç°‡å¤§å°è®¡ç®—èŠ‚ç‚¹æ•°é‡ï¼ˆ1-10ä¸ªèŠ‚ç‚¹ï¼‰
            label = angle_to_label[angle]
            cluster_size = cluster_sizes[leaves[i]]
            color = self.palette[label % len(self.palette)]
            
            # è®¡ç®—èŠ‚ç‚¹æ•°é‡ï¼šæ ¹æ®ç°‡å¤§å°æ¯”ä¾‹ï¼Œæœ€å°‘1ä¸ªï¼Œæœ€å¤š10ä¸ª
            max_cluster_size = max(cluster_sizes)
            if max_cluster_size > 0:
                node_count = max(1, min(10, int(round(cluster_size / max_cluster_size * 10))))
            else:
                node_count = 1
            
            # ç»Ÿä¸€èŠ‚ç‚¹å¤§å°
            node_size = 80
            
            # åœ¨è§’åº¦å‘¨å›´åˆ†å¸ƒå¤šä¸ªèŠ‚ç‚¹
            if node_count == 1:
                # å•ä¸ªèŠ‚ç‚¹åœ¨ä¸­å¿ƒ
                ax.scatter([angle], [leaf_radius], c=[color], s=node_size, 
                          edgecolors='white', linewidth=2, zorder=5, alpha=0.9)
            else:
                # å¤šä¸ªèŠ‚ç‚¹åœ¨è§’åº¦å‘¨å›´åˆ†å¸ƒ
                angle_spread = 0.2  # è§’åº¦åˆ†å¸ƒèŒƒå›´ï¼ˆå¼§åº¦ï¼‰
                radius_spread = 0.03  # åŠå¾„åˆ†å¸ƒèŒƒå›´
                
                # è®¡ç®—èŠ‚ç‚¹åˆ†å¸ƒ
                if node_count <= 3:
                    # å°‘é‡èŠ‚ç‚¹ï¼šçº¿æ€§åˆ†å¸ƒ
                    for j in range(node_count):
                        offset = (j - (node_count-1)/2) * angle_spread / max(1, node_count-1)
                        node_angle = angle + offset
                        node_radius = leaf_radius + (np.random.random() - 0.5) * radius_spread * 0.5
                        
                        ax.scatter([node_angle], [node_radius], c=[color], s=node_size, 
                                  edgecolors='white', linewidth=2, zorder=5, alpha=0.9)
                else:
                    # å¤šèŠ‚ç‚¹ï¼šç½‘æ ¼åˆ†å¸ƒ
                    grid_size = int(np.ceil(np.sqrt(node_count)))
                    for j in range(node_count):
                        row = j // grid_size
                        col = j % grid_size
                        
                        # åœ¨ç½‘æ ¼ä¸­è®¡ç®—ç›¸å¯¹ä½ç½®
                        rel_x = (col - (grid_size-1)/2) / max(1, grid_size-1)
                        rel_y = (row - (grid_size-1)/2) / max(1, grid_size-1)
                        
                        # è½¬æ¢ä¸ºæåæ ‡åç§»
                        node_angle = angle + rel_x * angle_spread
                        node_radius = leaf_radius + rel_y * radius_spread
                        
                        ax.scatter([node_angle], [node_radius], c=[color], s=node_size, 
                                  edgecolors='white', linewidth=2, zorder=5, alpha=0.9)
            
            # æ·»åŠ ç°‡æ ‡ç­¾
            label_text = f'Cluster {label} ({cluster_size})'
            # è°ƒæ•´æ–‡æœ¬ä½ç½®é¿å…é‡å 
            text_angle = np.degrees(angle)
            if text_angle > 90 and text_angle < 270:
                text_angle += 180
                ha = 'right'
            else:
                ha = 'left'
            
            ax.text(angle, leaf_radius + 0.15, label_text, 
                   ha=ha, va='center', fontsize=10, fontweight='bold',
                   color=color, transform=ax.transData)
        
        # ä¸­å¿ƒèŠ‚ç‚¹
        ax.scatter([0], [0], c='#333333', s=200, zorder=6, alpha=0.8)
        
        # è®¾ç½®æ˜¾ç¤ºèŒƒå›´
        ax.set_rlim(0, 1.2)
        
        if save_path:
            fig.savefig(save_path, format='svg', bbox_inches='tight', dpi=300)
            logger.info(f"Radial tree saved to {save_path}")
            
            # å¯¼å‡ºé“¾æ¥çŸ©é˜µå’Œç°‡ä¿¡æ¯CSV
            csv_path = Path(save_path).with_suffix('.csv')
            with open(csv_path, 'w', newline='') as cf:
                writer = csv.writer(cf)
                writer.writerow(["cluster_id", "size", "angle_degrees", "x_coord", "y_coord"])
                for i, angle in enumerate(angles):
                    label = angle_to_label[angle]
                    x_coord = leaf_radius * np.cos(angle)
                    y_coord = leaf_radius * np.sin(angle)
                    writer.writerow([int(label), int(cluster_sizes[leaves[i]]), 
                                   float(np.degrees(angle)), float(x_coord), float(y_coord)])
            
            # å¯¼å‡ºå±‚æ¬¡èšç±»é“¾æ¥çŸ©é˜µ
            linkage_csv = Path(save_path).with_suffix('_linkage.csv')
            with open(linkage_csv, 'w', newline='') as cf:
                writer = csv.writer(cf)
                writer.writerow(["idx1", "idx2", "distance", "sample_count"])
                for r in Z:
                    writer.writerow([int(r[0]), int(r[1]), float(r[2]), int(r[3])])
        
        if show_plot:
            plt.show()
        else:
            plt.close(fig)


# ==========================================================
# (11) ä¸»å‡½æ•°å…¥å£
# ==========================================================
def main():
    CIF_DIR = "./your_structure_dir"
    analyzer = ProteinClusterAnalyzer(CIF_DIR, antibody_chain='A', antigen_chains=['B','C'], dist_cutoff=5.0)
    logger.info("Starting analysis pipeline ...")
    try:
        ok = analyzer.load_and_process_data()
        if not ok:
            logger.error("Data loading/processing produced no results. Please check CIF_DIR and input files.")
            return

        X = analyzer.prepare_features()
        if X.size == 0:
            logger.error("Feature matrix is empty. Aborting pipeline.")
            return

        # ğŸ‘‰ é€‰æ‹©èšç±»æ–¹æ³•ï¼šhdbscan / kmeans / dbscan
        # åœ¨æ­¤ç»Ÿä¸€æ§åˆ¶ä¸‰ç§ç®—æ³•æ˜¯å¦ä½¿ç”¨è‡ªé€‚åº”å‚æ•°æˆ–æ‰‹åŠ¨å‚æ•°
        cluster_method = "kmeans"  # å¯é€‰: "hdbscan" | "kmeans" | "dbscan"

        # ç»Ÿä¸€çš„å‚æ•°æ§åˆ¶å¼€å…³ï¼šauto=True èµ°è‡ªé€‚åº”ï¼›auto=False ä½¿ç”¨ä¸‹æ–¹ç»™å®šå‚æ•°
        cluster_params = {
            "kmeans": {
                "auto": True,       # True: è‡ªåŠ¨é€‰æ‹©ç°‡æ•°ï¼›False: ä½¿ç”¨ n_clusters
                "n_clusters": 4
            },
            "hdbscan": {
                "auto": True,       # True: è‡ªé€‚åº” min_cluster_size/min_samplesï¼›False: ä½¿ç”¨ä¸‹æ–¹è®¾å®š
                "min_cluster_size": 10,
                "min_samples": 5,
                "cluster_selection_epsilon": 0.1
            },
            "dbscan": {
                "auto": True,       # True: è‡ªåŠ¨ä¼°è®¡ eps/min_samplesï¼›False: ä½¿ç”¨ä¸‹æ–¹è®¾å®š
                "eps": 0.5,
                "min_samples": 5
            }
        }

        # æ„é€ ä¼ å…¥ perform_clustering çš„ kwargs
        kwargs = {}
        if cluster_method == "kmeans":
            if not cluster_params["kmeans"]["auto"]:
                kwargs["n_clusters"] = cluster_params["kmeans"]["n_clusters"]
                logger.info(f"KMeans manual params -> n_clusters={kwargs['n_clusters']}")
            else:
                logger.info("KMeans auto mode -> auto-select n_clusters by silhouette")
        elif cluster_method == "hdbscan":
            if not cluster_params["hdbscan"]["auto"]:
                kwargs["min_cluster_size"] = cluster_params["hdbscan"]["min_cluster_size"]
                kwargs["min_samples"] = cluster_params["hdbscan"]["min_samples"]
                kwargs["cluster_selection_epsilon"] = cluster_params["hdbscan"]["cluster_selection_epsilon"]
                logger.info(
                    f"HDBSCAN manual params -> min_cluster_size={kwargs['min_cluster_size']}, "
                    f"min_samples={kwargs['min_samples']}, eps={kwargs['cluster_selection_epsilon']}"
                )
            else:
                logger.info("HDBSCAN auto mode -> estimate min_cluster_size/min_samples")
        elif cluster_method == "dbscan":
            if not cluster_params["dbscan"]["auto"]:
                kwargs["eps"] = cluster_params["dbscan"]["eps"]
                kwargs["min_samples"] = cluster_params["dbscan"]["min_samples"]
                logger.info(f"DBSCAN manual params -> eps={kwargs['eps']}, min_samples={kwargs['min_samples']}")
            else:
                logger.info("DBSCAN auto mode -> estimate eps/min_samples")

        analyzer.perform_clustering(X, method=cluster_method, **kwargs)

        # æ„å»ºè¾“å‡ºç›®å½•ä¸æ–‡ä»¶å‘½åï¼šè¾“å…¥æ–‡ä»¶å¤¹å + clusteræ–¹æ³• + æ—¥æœŸ
        input_base = Path(CIF_DIR).resolve().name
        date_str = datetime.now().strftime("%Y%m%d_%H%M%S")
        output_dir = Path(f"{input_base}_{cluster_method}_{date_str}")
        output_dir.mkdir(parents=True, exist_ok=True)

        # è¯„ä¼° + ä¿å­˜ + è¾“å‡ºä»£è¡¨
        metrics = analyzer.evaluate_clustering(X)
        logger.info(f"Clustering metrics: {metrics}")
        results_pkl = output_dir / f"{input_base}_{cluster_method}_{date_str}_results.pkl"
        analyzer.save_results(results_pkl, X)
        reps = analyzer.get_cluster_representatives(X)
        logger.info(f"Representatives: {reps}")

        # å¯è§†åŒ–
        viz_path = output_dir / f"{input_base}_{cluster_method}_{date_str}_clustering.png"
        analyzer.visualize_results(X, save_path=viz_path, show_plot=False)

        # å¾„å‘æ ‘å›¾ï¼ˆç¬¬äºŒå¼ å›¾é£æ ¼ï¼‰
        radial_svg = output_dir / f"{input_base}_{cluster_method}_{date_str}_radial_tree.svg"
        analyzer.plot_cluster_radial_tree(X, save_path=radial_svg, show_plot=False)
        logger.info("All tasks finished. Exiting.")
    except Exception:
        logger.exception("Analysis failed with an unexpected error")


if __name__ == "__main__":
    main()
